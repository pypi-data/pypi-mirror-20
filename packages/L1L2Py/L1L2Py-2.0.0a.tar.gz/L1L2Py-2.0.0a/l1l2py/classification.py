"""Wrapper for l1l2 in case of classification."""
from sklearn.linear_model.base import LinearClassifierMixin
from sklearn.feature_selection.base import SelectorMixin
from sklearn.preprocessing import LabelBinarizer
from sklearn.utils.validation import check_is_fitted
from sklearn.feature_selection.from_model import _get_feature_importances
from sklearn.feature_selection.from_model import _calculate_threshold

from l1l2py.linear_model import L1L2


class L1L2Classifier(LinearClassifierMixin, SelectorMixin, L1L2):
    """Classification with combined L1 and L2 priors as regularizer.

    Minimizes the objective function::
            1 / n_samples * ||y - Xw||^2_2
            + tau * ||w||_1
            + mu * ||w||^2_2

    using the FISTA method, Dealing with classification problems.

    Parameters
    ----------
    mu : float, optional, default 0.5
        Constant that multiplies the l1 norm.

    tau : float, optional, default 1
        Constant that multiplies the l2 norm.

    use_gpu : bool, optional, default False
        If True, use the implementation of FISTA using the GPU.
        Currently ignored.

    threshold : float, optional, default 1e-16
        Threshold to select relevant variables in the ``transform`` method.

    alpha : float, optional, default None
        Constant that multiplies the penalty terms. Defaults to None.
        This is for parallel with sklearn ElasticNet class.
        See the notes for the exact mathematical meaning of this
        parameter.``alpha = 0`` is equivalent to an ordinary least square,
        solved by the :class:`LinearRegression` object. For numerical
        reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.
        Given this, you should use the :class:`LinearRegression` object.

    l1_ratio : float, optional, default None
        This is for parallel with sklearn ElasticNet class.
        The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For
        ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it
        is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a
        combination of L1 and L2.

    fit_intercept : bool
        Whether the intercept should be estimated or not. If ``False``, the
        data is assumed to be already centered.

    normalize : boolean, optional, default False
        If ``True``, the regressors X will be normalized before regression.
        This parameter is ignored when ``fit_intercept`` is set to ``False``.
        When the regressors are normalized, note that this makes the
        hyperparameters learnt more robust and almost independent of the number
        of samples. The same property is not valid for standardized data.
        However, if you wish to standardize, please use
        :class:`preprocessing.StandardScaler` before calling ``fit`` on an
        estimator with ``normalize=False``.

    precompute : True | False | array-like
        Whether to use a precomputed Gram matrix to speed up
        calculations. The Gram matrix can also be passed as argument.
        For sparse input this option is always ``True`` to preserve sparsity.

    max_iter : int, optional
        The maximum number of iterations

    copy_X : boolean, optional, default True
        If ``True``, X will be copied; else, it may be overwritten.

    tol : float, optional
        The tolerance for the optimization: if the updates are
        smaller than ``tol``, the optimization code checks the
        dual gap for optimality and continues until it is smaller
        than ``tol``.

    warm_start : bool, optional
        When set to ``True``, reuse the solution of the previous call to fit as
        initialization, otherwise, just erase the previous solution.

    positive : bool, optional
        When set to ``True``, forces the coefficients to be positive.

    selection : str, default 'cyclic'
        If set to 'random', a random coefficient is updated every iteration
        rather than looping over features sequentially by default. This
        (setting to 'random') often leads to significantly faster convergence
        especially when tol is higher than 1e-4.

    random_state : int, RandomState instance, or None (default)
        The seed of the pseudo random number generator that selects
        a random feature to update. Useful only when selection is set to
        'random'.

    Attributes
    ----------
    coef_ : array, shape (n_features,) | (n_targets, n_features)
        parameter vector (w in the cost function formula)

    sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) | \
            (n_targets, n_features)
        ``sparse_coef_`` is a readonly property derived from ``coef_``

    intercept_ : float | array, shape (n_targets,)
        independent term in decision function.

    n_iter_ : array-like, shape (n_targets,)
        number of iterations run by the coordinate descent solver to reach
        the specified tolerance.
    """

    def __init__(self, mu=.5, tau=1.0, use_gpu=False, threshold=1e-16,
                 alpha=None, l1_ratio=None, fit_intercept=True,
                 normalize=False, precompute=False, max_iter=10000,
                 copy_X=True, tol=1e-4, warm_start=False, positive=False,
                 random_state=None, selection='cyclic'):
        """
        Parameters
        ----------

        """
        self.mu = mu
        self.tau = tau
        self.use_gpu = use_gpu
        self.threshold = threshold  # threshold to select relevant feature

        self.alpha = alpha
        self.l1_ratio = l1_ratio
        self.coef_ = None
        self.fit_intercept = fit_intercept
        self.normalize = normalize
        self.precompute = precompute
        self.max_iter = max_iter
        self.copy_X = copy_X
        self.tol = tol
        self.warm_start = warm_start
        self.positive = positive
        self.intercept_ = 0.0
        self.random_state = random_state
        self.selection = selection

    def fit(self, X, y, check_input=True):
        """Fit model with fista.

        Parameters
        -----------
        X : ndarray or scipy.sparse matrix, (n_samples, n_features)
            Data

        y : ndarray, shape (n_samples,) or (n_samples, n_targets)
            Target

        check_input : boolean, (default=True)
            Allow to bypass several input checking.
            Don't use this parameter unless you know what you do.
        """
        if self.l1_ratio is not None and self.alpha is not None:
            # tau and mu are selected as enet
            if self.l1_ratio == 1:
                self.mu = 0
                self.tau = 2 * self.alpha
            elif self.l1_ratio == 0:
                self.mu = 2 * self.alpha
                self.tau = 0
            else:
                self.mu = 2 * self.alpha * (1 - self.l1_ratio)
                self.tau = 2 * self.alpha * self.l1_ratio
        else:
            self.l1_ratio = self.tau / (self.tau + 2 * self.mu)
            self.alpha = self.tau * .5 / self.l1_ratio

        self._label_binarizer = LabelBinarizer(pos_label=1, neg_label=-1)
        y = self._label_binarizer.fit_transform(y)
        if self._label_binarizer.y_type_.startswith('multilabel'):
            raise ValueError(
                "%s doesn't support multi-label classification" % (
                    self.__class__.__name__))

        # self.coef_ = self.path(
        #     X, y, self.mu, self.tau, beta=None, kmax=self.max_iter,
        #     tolerance=self.tol, return_iterations=False, adaptive=False)
        super(L1L2Classifier, self).fit(X, y, check_input)

        if self.classes_.shape[0] > 2:
            ndim = self.classes_.shape[0]
        else:
            ndim = 1
        self.coef_ = self.coef_.reshape(ndim, -1)

        return self

    @property
    def classes_(self):
        return self._label_binarizer.classes_

    def _get_support_mask(self):
        check_is_fitted(self, "n_iter_")
        scores = _get_feature_importances(self)
        self.threshold_ = _calculate_threshold(self, scores, self.threshold)
        return scores >= self.threshold_
